<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Off the convex path</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Algorithms off the convex path.">
    <meta name="author" content="Moritz Hardt">
    
    <link rel="canonical" href="http://offconvex.github.io/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Off the convex path" href="/feed.xml" />
    <link rel="stylesheet" href="/css/pixyll.css?201707190818" type="text/css">

    <!-- Fonts -->
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css" rel="stylesheet">
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Off the convex path">
    <meta property="og:description" content="Algorithms off the convex path.">
    <meta property="og:url" content="http://offconvex.github.io/">
    <meta property="og:site_name" content="Off the convex path">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
          processEscapes: true
        },
        messageStyle: "none",
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
    </script>

</head>

<body class="site">

	

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="http://offconvex.github.io" class="site-title">
      <img style="width:500px;" src="/assets/logo.jpg" />
      </a>
      <nav class="site-nav" style="padding-top:200px;">
        <a href="/about/">About</a>
<a href="/contact/">Contact</a>
<a href="/subscribe/">Subscribe</a>

      </nav>
      <div class="clearfix"></div>
      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        <div class="home">

  <div class="posts">
    
     
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2017/07/19/saddle-efficiency/" class="post-link">
        How to Escape Saddle Points Efficiently
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jul 19, 2017.&nbsp;&nbsp;</span>
A core, emerging problem in nonconvex optimization involves the escape of saddlepoints. While recent research has shown that gradient descent (GD) generically escapes saddle points asymptotically (see Rong Ge’s and Ben Recht’s blog posts), the... <a href="/2017/07/19/saddle-efficiency/">Continue</a>
        </p>
      </div>
     
    
     
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2017/07/06/GANs3/" class="post-link">
        Do GANs actually do distribution learning?
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jul 6, 2017.&nbsp;&nbsp;</span>
This post is about our new paper, which presents empirical evidence that current GANs (Generative Adversarial Nets) are quite far from learning the target distribution. Previous posts had introduced GANs and described new theoretical analysis... <a href="/2017/07/06/GANs3/">Continue</a>
        </p>
      </div>
     
    
     
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2017/06/26/unsupervised1/" class="post-link">
        Unsupervised learning, one notion or many?
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jun 26, 2017.&nbsp;&nbsp;</span>
Unsupervised learning, as the name suggests, is the science of learning from unlabeled data. A look at the wikipedia page shows that this term has many interpretations: (Task A) Learning a distribution from samples. (Examples:... <a href="/2017/06/26/unsupervised1/">Continue</a>
        </p>
      </div>
     
    
     
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2017/03/30/GANs2/" class="post-link">
        Generalization and Equilibrium in Generative Adversarial Networks (GANs)
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Mar 30, 2017.&nbsp;&nbsp;</span>
The previous post described Generative Adversarial Networks (GANs), a technique for training generative models for image distributions (and other complicated distributions) via a 2-party game between a generator deep net and a discriminator deep net.... <a href="/2017/03/30/GANs2/">Continue</a>
        </p>
      </div>
     
    
     
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2017/03/15/GANs/" class="post-link">
        Generative Adversarial Networks (GANs), Some Open Questions
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Mar 15, 2017.&nbsp;&nbsp;</span>
Since ability to generate “realistic-looking” data may be a step towards understanding its structure and exploiting it, generative models are an important component of unsupervised learning, which has been a frequent theme on this blog.... <a href="/2017/03/15/GANs/">Continue</a>
        </p>
      </div>
     
    
     
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/12/20/backprop/" class="post-link">
        Back-propagation, an introduction
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Dec 20, 2016.&nbsp;&nbsp;</span>
Given the sheer number of backpropagation tutorials on the internet, is there really need for another? One of us (Sanjeev) recently taught backpropagation in undergrad AI and couldn’t find any account he was happy with.... <a href="/2016/12/20/backprop/">Continue</a>
        </p>
      </div>
     
    
     
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/11/03/MityaNN1/" class="post-link">
        The search for biologically plausible neural computation&#58; The conventional approach
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Nov 3, 2016.&nbsp;&nbsp;</span>
Inventors of the original artificial neural networks (NNs) derived their inspiration from biology. However, as artificial NNs progressed, their design was less guided by neuroscience facts. Meanwhile, progress in neuroscience has altered our conceptual understanding... <a href="/2016/11/03/MityaNN1/">Continue</a>
        </p>
      </div>
     
    
     
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/10/13/gradient-descent-learns-dynamical-systems/" class="post-link">
        Gradient Descent Learns Linear Dynamical Systems
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Oct 13, 2016.&nbsp;&nbsp;</span>
From text translation to video captioning, learning to map one sequence to another is an increasingly active research area in machine learning. Fueled by the success of recurrent neural networks in its many variants, the... <a href="/2016/10/13/gradient-descent-learns-dynamical-systems/">Continue</a>
        </p>
      </div>
     
    
     
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/07/10/embeddingspolysemy/" class="post-link">
        Linear algebraic structure of word meanings
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jul 10, 2016.&nbsp;&nbsp;</span>
Word embeddings capture the meaning of a word using a low-dimensional vector and are ubiquitous in natural language processing (NLP). (See my earlier post 1 and post2.) It has always been unclear how to interpret... <a href="/2016/07/10/embeddingspolysemy/">Continue</a>
        </p>
      </div>
     
    
     
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/05/08/almostconvexitySATM/" class="post-link">
        A Framework for analysing Non-Convex Optimization
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">May 8, 2016.&nbsp;&nbsp;</span>
Previously Rong’s post and Ben’s post show that (noisy) gradient descent can converge to local minimum of a non-convex function, and in (large) polynomial time (Ge et al.’15). This post describes a simple framework that... <a href="/2016/05/08/almostconvexitySATM/">Continue</a>
        </p>
      </div>
     
    
  </div>

  <div class="pagination clearfix mb1 mt4">
  <div class="left">
    
      <span class="pagination-item disabled">Newer</span>
    
  </div>
  <div class="right">
    
      <a class="pagination-item" href="/page2/">Older</a>
    
  </div>
</div>

</div>

      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
    <small>
      Theme available on <a href="https://github.com/johnotander/pixyll">Github</a>.
    </small>
  </div>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-70478681-1', 'auto');
    ga('send', 'pageview');
  </script>
</footer>

</body>
</html>
